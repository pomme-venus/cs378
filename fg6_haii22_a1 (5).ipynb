{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Loan Prediction\n",
    "\n",
    "The goal of this task is to build a predictive machine learning classifier that will use historic data of loan decisions (ground truth) to make recommended decisions for new loan applications. Along the way, you’ll get comfortable with using Jupyter Notebooks to document your work, using scikit-learn to train and test basic ML models, and start thinking about the strengths and weaknesses of models like this.\n",
    "\n",
    "The dataset `home_loans_sample.csv` a listing of home loan applications in Washington, USA, where each row of the dataset is an individual loan application. Your goal in this assignment is to build a machine learning model that can accurately predict whether a given loan application was accepted or rejected. \n",
    "\n",
    "## Background\n",
    "In the US (and a number of other countries), it is common for people who want to buy a home to apply for a loan (called a “mortgage”) from a bank or other lender. In order for the bank to decide whether to give them this loan, the bank will look at a number of factors to determine whether they are likely to be able to repay the loan because the bank does not want to lend money to somebody who would not be able to pay it back. These factors typically include how much money the person makes, how much money they are asking for in the loan, and a handful of other things.  Banks typically have formulas in place to determine who they will approve for a given loan and who they will deny, but there is also frequently some human judgment involved in the process.\n",
    "\n",
    "If the person is approved for the loan, they will use the loan to purchase the home and will pay it back in small chunks every month for a set period of time (usually 15 or 30 years). If they are denied, they may have to find a less expensive home to buy or address some of the factors that led them to be denied (e.g., they might have to wait until they make more money). \n",
    "\n",
    "In this task, we are given a set of data about whether customers were approved or denied a loan based on a number of factors. In this assignment, your job will be to build a model that predicts whether a person will be approved or denied based on these same factors.\n",
    "\n",
    "\n",
    "## Part 1: Data Exploration\n",
    "The first few exercises will get you used to looking at the data using `pandas`. Pandas is a widely used library in python for manipulating data. Why? Datasets can consume a _lot_ of space in your computer's memory and traditional python data structures like lists or dictionaries will become painfully slow as we add thousands of rows of data. We use a specialized dataset library `pandas` which has a specialized data structure called a `dataframe` designed to be ultra fast & efficient. Documentation is here: https://pandas.pydata.org/pandas-docs/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas library\n",
    "df = pd.read_csv('data/home_loans_sample.csv', low_memory=False) # read the csv file into a pandas dataframe object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what kind of data was collected, `pandas` has some handy commands:\n",
    "\n",
    "- `df.head()` will show us the first 5 rows of our dataset. You can also specify the first N rows, like `df.head(18)` will show us the first 18 rows.\n",
    "- `df.sample(10)` will show us 10 randomly sampled rows of our dataset (but this may not show all the columns!)\n",
    "- `df.shape` will tell us how many rows and how many columns are in the dataset\n",
    "- `df.columns` will list the names of all columns in the dataset\n",
    "- `df.describe()` will give you summary statistics about all numerical columns in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.a:  How many rows are in this dataset? How many columns?\n",
    "_Double click this text to write your answer to the question here. Show your python work in the code block (`In [  ]:`) below:\n",
    "\n",
    "27 columns and 369281 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 columns and 369281 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"{} columns and {} rows\".format(len(df.columns),len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b: One of the columns in the dataset is the outcome value for each application, the value we will try to predict. Which column is that?\n",
    "_Double click this text to write your answer to the question here. Show your python work in the code block below, if applicable:\n",
    "\n",
    "10th column : loan_approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['town_name', 'county_name', 'loan_amount_000s', 'applicant_income_000s',\n",
      "       'property_type_name', 'occupied_by_owner', 'loan_type_name',\n",
      "       'is_hoepa_loan', 'loan_purpose_name', 'loan_approved',\n",
      "       'denial_reason_name_3', 'denial_reason_name_2', 'denial_reason_name_1',\n",
      "       'co_applicant_sex_name', 'co_applicant_race_name_5',\n",
      "       'co_applicant_race_name_4', 'co_applicant_race_name_3',\n",
      "       'co_applicant_race_name_2', 'co_applicant_race_name_1',\n",
      "       'co_applicant_ethnicity_name', 'applicant_sex_name',\n",
      "       'applicant_race_name_5', 'applicant_race_name_4',\n",
      "       'applicant_race_name_3', 'applicant_race_name_2',\n",
      "       'applicant_race_name_1', 'applicant_ethnicity_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.c: What reasons were given in this dataset for denying a loan application?\n",
    "Hint: There are 3 columns in the dataset that list why a loan was denied. Try looking up the pandas command to list the unique values in a column.\n",
    "\n",
    "_Double click this text to write your answer to the question here. Show your python work in the code block below:_\n",
    "\n",
    "nan, 'Other', 'Credit history',\n",
    "       'Insufficient cash (downpayment, closing costs)',\n",
    "       'Employment history', 'Debt-to-income ratio',\n",
    "       'Unverifiable information', 'Collateral',\n",
    "       'Credit application incomplete', 'Mortgage insurance denied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Other', 'Credit history',\n",
       "       'Insufficient cash (downpayment, closing costs)',\n",
       "       'Employment history', 'Debt-to-income ratio',\n",
       "       'Unverifiable information', 'Collateral',\n",
       "       'Credit application incomplete', 'Mortgage insurance denied'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['denial_reason_name_1'].unique()\n",
    "df['denial_reason_name_2'].unique()\n",
    "df['denial_reason_name_3'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.d: Given the denial reasons columns and the rest of the columns in this dataset, think about what information you _don't_ have about each application. Rank your top 3 _missing_ pieces of information about each application that could help you better predict the application's loan outcome.\n",
    "_Double click this text to write your answer to the question here. Show your python work in the code block below, if applicable:_\n",
    "\n",
    "#1. Renting history\n",
    "\n",
    "#2. Criminal history\n",
    "\n",
    "#3. Marital status / number of people living with the person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['town_name', 'county_name', 'loan_amount_000s', 'applicant_income_000s',\n",
       "       'property_type_name', 'occupied_by_owner', 'loan_type_name',\n",
       "       'is_hoepa_loan', 'loan_purpose_name', 'loan_approved',\n",
       "       'denial_reason_name_3', 'denial_reason_name_2', 'denial_reason_name_1',\n",
       "       'co_applicant_sex_name', 'co_applicant_race_name_5',\n",
       "       'co_applicant_race_name_4', 'co_applicant_race_name_3',\n",
       "       'co_applicant_race_name_2', 'co_applicant_race_name_1',\n",
       "       'co_applicant_ethnicity_name', 'applicant_sex_name',\n",
       "       'applicant_race_name_5', 'applicant_race_name_4',\n",
       "       'applicant_race_name_3', 'applicant_race_name_2',\n",
       "       'applicant_race_name_1', 'applicant_ethnicity_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Data to Input to a Model\n",
    "Here we'll start using `scikit-learn` which provides simple library calls for most things we'd like to do in a simple machine learning pipeline. If you haven't used `scikit-learn` before this tutorial may be useful to give you a sense of what the library can do: https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "Machine learning models can only understand data that is represented numerically, but lots of the columns in our dataset like `town_name` are text _categorical_ data. Meanwhile, many models do better when continous numerical data is within small, consistent ranges, such as all data being between -1, 0 and 1, which is definitely not the case with our thousands of dollars loan units.\n",
    "\n",
    "So first, we will separate out our samples (called _X_) into features (sometimes called attributes) we'd like to include in our model that are categorical or continous so that we can preprocess each appropriately, separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_sex_name', 'town_name',\n",
    "                'county_name']\n",
    "\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s', 'is_hoepa_loan']\n",
    "\n",
    "X_cat = df[features_cat]\n",
    "X_num = df[features_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.A One Hot Encode categorical variables\n",
    "Run the following code to One Hot Encode the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_purpose_name_Home improvement</th>\n",
       "      <th>loan_purpose_name_Home purchase</th>\n",
       "      <th>loan_purpose_name_Refinancing</th>\n",
       "      <th>applicant_sex_name_Female</th>\n",
       "      <th>applicant_sex_name_Information not provided by applicant in mail, Internet, or telephone application</th>\n",
       "      <th>applicant_sex_name_Male</th>\n",
       "      <th>applicant_sex_name_Not applicable</th>\n",
       "      <th>town_name_Bellingham - WA</th>\n",
       "      <th>town_name_Bremerton, Silverdale - WA</th>\n",
       "      <th>town_name_Kennewick, Richland - WA</th>\n",
       "      <th>...</th>\n",
       "      <th>county_name_Snohomish County</th>\n",
       "      <th>county_name_Spokane County</th>\n",
       "      <th>county_name_Stevens County</th>\n",
       "      <th>county_name_Thurston County</th>\n",
       "      <th>county_name_Wahkiakum County</th>\n",
       "      <th>county_name_Walla Walla County</th>\n",
       "      <th>county_name_Whatcom County</th>\n",
       "      <th>county_name_Whitman County</th>\n",
       "      <th>county_name_Yakima County</th>\n",
       "      <th>county_name_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_purpose_name_Home improvement  loan_purpose_name_Home purchase  \\\n",
       "0                                 0.0                              0.0   \n",
       "1                                 0.0                              1.0   \n",
       "2                                 0.0                              0.0   \n",
       "3                                 0.0                              0.0   \n",
       "4                                 1.0                              0.0   \n",
       "\n",
       "   loan_purpose_name_Refinancing  applicant_sex_name_Female  \\\n",
       "0                            1.0                        1.0   \n",
       "1                            0.0                        0.0   \n",
       "2                            1.0                        0.0   \n",
       "3                            1.0                        0.0   \n",
       "4                            0.0                        1.0   \n",
       "\n",
       "   applicant_sex_name_Information not provided by applicant in mail, Internet, or telephone application  \\\n",
       "0                                                0.0                                                      \n",
       "1                                                0.0                                                      \n",
       "2                                                0.0                                                      \n",
       "3                                                0.0                                                      \n",
       "4                                                0.0                                                      \n",
       "\n",
       "   applicant_sex_name_Male  applicant_sex_name_Not applicable  \\\n",
       "0                      0.0                                0.0   \n",
       "1                      1.0                                0.0   \n",
       "2                      1.0                                0.0   \n",
       "3                      1.0                                0.0   \n",
       "4                      0.0                                0.0   \n",
       "\n",
       "   town_name_Bellingham - WA  town_name_Bremerton, Silverdale - WA  \\\n",
       "0                        0.0                                   0.0   \n",
       "1                        0.0                                   0.0   \n",
       "2                        0.0                                   0.0   \n",
       "3                        0.0                                   0.0   \n",
       "4                        0.0                                   1.0   \n",
       "\n",
       "   town_name_Kennewick, Richland - WA  ...  county_name_Snohomish County  \\\n",
       "0                                 0.0  ...                           0.0   \n",
       "1                                 0.0  ...                           0.0   \n",
       "2                                 0.0  ...                           0.0   \n",
       "3                                 0.0  ...                           0.0   \n",
       "4                                 0.0  ...                           0.0   \n",
       "\n",
       "   county_name_Spokane County  county_name_Stevens County  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   county_name_Thurston County  county_name_Wahkiakum County  \\\n",
       "0                          0.0                           0.0   \n",
       "1                          0.0                           0.0   \n",
       "2                          0.0                           0.0   \n",
       "3                          0.0                           0.0   \n",
       "4                          0.0                           0.0   \n",
       "\n",
       "   county_name_Walla Walla County  county_name_Whatcom County  \\\n",
       "0                             0.0                         0.0   \n",
       "1                             1.0                         0.0   \n",
       "2                             0.0                         0.0   \n",
       "3                             0.0                         0.0   \n",
       "4                             0.0                         0.0   \n",
       "\n",
       "   county_name_Whitman County  county_name_Yakima County  county_name_nan  \n",
       "0                         0.0                        0.0              0.0  \n",
       "1                         0.0                        0.0              0.0  \n",
       "2                         0.0                        0.0              0.0  \n",
       "3                         0.0                        0.0              0.0  \n",
       "4                         0.0                        0.0              0.0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pd.DataFrame(one_hot.toarray(), columns=enc.get_feature_names_out())\n",
    "X_cat_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.a: In your own words, how is one hot coding tranforming the categorical data? What does the term \"one-hot\" refer to?\n",
    "one hot encoding is the process of converting categorical data into numerical data (binary values, 0s and 1s). It is called one-hot because in each row, if something falls under any column category, it's going to be 1, and 0 otherwise. so we have an idea about the existence/nonexistence of different features with just 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.B Scaling down continuous numerical data\n",
    "Run the following code to normalize any continous data around the column's mean. This process will ensure that the average of that feature, such as the average amount that a person asks for in loan amount, is scaled to 0. Values less than the average will be negative numbers, and values larger than the average will be positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amount_000s</th>\n",
       "      <th>applicant_income_000s</th>\n",
       "      <th>is_hoepa_loan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130864</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>-0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103680</td>\n",
       "      <td>-0.596232</td>\n",
       "      <td>-0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101589</td>\n",
       "      <td>0.024727</td>\n",
       "      <td>-0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128424</td>\n",
       "      <td>1.664059</td>\n",
       "      <td>-0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266432</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.005701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amount_000s  applicant_income_000s  is_hoepa_loan\n",
       "0         -0.130864               0.016448      -0.005701\n",
       "1         -0.103680              -0.596232      -0.005701\n",
       "2         -0.101589               0.024727      -0.005701\n",
       "3          0.128424               1.664059      -0.005701\n",
       "4          0.266432              -0.000111      -0.005701"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaled = preprocessing.scale(X_num)\n",
    "X_num_proc = pd.DataFrame(scaled, columns=features_num)\n",
    "X_num_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.C Merge our feature sets into one sample dataset _X_ and fix NaN values\n",
    "Run the code below to combine the numerical and categorical feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amount_000s</th>\n",
       "      <th>applicant_income_000s</th>\n",
       "      <th>is_hoepa_loan</th>\n",
       "      <th>loan_purpose_name_Home improvement</th>\n",
       "      <th>loan_purpose_name_Home purchase</th>\n",
       "      <th>loan_purpose_name_Refinancing</th>\n",
       "      <th>applicant_sex_name_Female</th>\n",
       "      <th>applicant_sex_name_Information not provided by applicant in mail, Internet, or telephone application</th>\n",
       "      <th>applicant_sex_name_Male</th>\n",
       "      <th>applicant_sex_name_Not applicable</th>\n",
       "      <th>...</th>\n",
       "      <th>county_name_Snohomish County</th>\n",
       "      <th>county_name_Spokane County</th>\n",
       "      <th>county_name_Stevens County</th>\n",
       "      <th>county_name_Thurston County</th>\n",
       "      <th>county_name_Wahkiakum County</th>\n",
       "      <th>county_name_Walla Walla County</th>\n",
       "      <th>county_name_Whatcom County</th>\n",
       "      <th>county_name_Whitman County</th>\n",
       "      <th>county_name_Yakima County</th>\n",
       "      <th>county_name_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130864</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>-0.005701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103680</td>\n",
       "      <td>-0.596232</td>\n",
       "      <td>-0.005701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101589</td>\n",
       "      <td>0.024727</td>\n",
       "      <td>-0.005701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128424</td>\n",
       "      <td>1.664059</td>\n",
       "      <td>-0.005701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266432</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.005701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amount_000s  applicant_income_000s  is_hoepa_loan  \\\n",
       "0         -0.130864               0.016448      -0.005701   \n",
       "1         -0.103680              -0.596232      -0.005701   \n",
       "2         -0.101589               0.024727      -0.005701   \n",
       "3          0.128424               1.664059      -0.005701   \n",
       "4          0.266432              -0.000111      -0.005701   \n",
       "\n",
       "   loan_purpose_name_Home improvement  loan_purpose_name_Home purchase  \\\n",
       "0                                 0.0                              0.0   \n",
       "1                                 0.0                              1.0   \n",
       "2                                 0.0                              0.0   \n",
       "3                                 0.0                              0.0   \n",
       "4                                 1.0                              0.0   \n",
       "\n",
       "   loan_purpose_name_Refinancing  applicant_sex_name_Female  \\\n",
       "0                            1.0                        1.0   \n",
       "1                            0.0                        0.0   \n",
       "2                            1.0                        0.0   \n",
       "3                            1.0                        0.0   \n",
       "4                            0.0                        1.0   \n",
       "\n",
       "   applicant_sex_name_Information not provided by applicant in mail, Internet, or telephone application  \\\n",
       "0                                                0.0                                                      \n",
       "1                                                0.0                                                      \n",
       "2                                                0.0                                                      \n",
       "3                                                0.0                                                      \n",
       "4                                                0.0                                                      \n",
       "\n",
       "   applicant_sex_name_Male  applicant_sex_name_Not applicable  ...  \\\n",
       "0                      0.0                                0.0  ...   \n",
       "1                      1.0                                0.0  ...   \n",
       "2                      1.0                                0.0  ...   \n",
       "3                      1.0                                0.0  ...   \n",
       "4                      0.0                                0.0  ...   \n",
       "\n",
       "   county_name_Snohomish County  county_name_Spokane County  \\\n",
       "0                           0.0                         0.0   \n",
       "1                           0.0                         0.0   \n",
       "2                           0.0                         0.0   \n",
       "3                           0.0                         0.0   \n",
       "4                           0.0                         0.0   \n",
       "\n",
       "   county_name_Stevens County  county_name_Thurston County  \\\n",
       "0                         0.0                          0.0   \n",
       "1                         0.0                          0.0   \n",
       "2                         0.0                          0.0   \n",
       "3                         0.0                          0.0   \n",
       "4                         0.0                          0.0   \n",
       "\n",
       "   county_name_Wahkiakum County  county_name_Walla Walla County  \\\n",
       "0                           0.0                             0.0   \n",
       "1                           0.0                             1.0   \n",
       "2                           0.0                             0.0   \n",
       "3                           0.0                             0.0   \n",
       "4                           0.0                             0.0   \n",
       "\n",
       "   county_name_Whatcom County  county_name_Whitman County  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   county_name_Yakima County  county_name_nan  \n",
       "0                        0.0              0.0  \n",
       "1                        0.0              0.0  \n",
       "2                        0.0              0.0  \n",
       "3                        0.0              0.0  \n",
       "4                        0.0              0.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.c The code line below removes any NaN values in our sample with 0. NaNs are missing values that a model won't be able to understand. What is the _semantic_ meaning of replacing a NaN with 0 for the categorical variables? And for the continous numerical variables? \n",
    "\n",
    "Since we're using one-hot encoding to convert our variables into binary features, for categorical variables, a row that has a 0 for all the \"options\" of a certain catergory (i.e race) means that we don't have that information and it won't be taken into consideration as a factor of deciding whether or not to give the person a loan. For the continuous numerical variables, it indicates non-missing data that is actually equal 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(0) # remove NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.D Create our target array _y_ that our model will try to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['loan_approved'] # target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.E Split our data into training, test, and validation sets\n",
    "Run the code below to split the data. Both validation and test sets will be used for testing our model, but use the validation set while you are developing and improving your model, and leave the test for late stage evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258496, 65) (55392, 65) (55393, 65)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.30) # split out into training 70% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.50,  random_state=16) # split out into validation 15% of our data and test 15% of our data\n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # print data shape to check the sizing is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.e:  In a  single sentence, what is the difference between train, test, and validation sets?\n",
    "\n",
    "\n",
    "_the Training set_ is to build up our ML model, _the validation set_ is a subset of our original data set that helps us evaluate the model's performance as we train it and tweak it, and _the testing set_ is our final step in evaluating our model's peformance on data it hasn't seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Developing Models\n",
    "Scikit-learn has a substantial library of different models we can use for classification. Below are implemented two of the most simple classification models, Logistic Regression and Dummy Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[   24  9041]\n",
      " [    8 46319]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.00      0.01      9065\n",
      "           1       0.84      1.00      0.91     46327\n",
      "\n",
      "    accuracy                           0.84     55392\n",
      "   macro avg       0.79      0.50      0.46     55392\n",
      "weighted avg       0.82      0.84      0.76     55392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs',max_iter=1000).fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred = model.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred) # finally evaluate performance\n",
    "\n",
    "#how is the precision = 1 ? solved in office hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dummy Classifier is a 'dummy' because it is going to use zero machine learning, and simply predict \"approve this loan\" (value 1) for every loan it sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[    0  9065]\n",
      " [    0 46327]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9065\n",
      "           1       0.84      1.00      0.91     46327\n",
      "\n",
      "    accuracy                           0.84     55392\n",
      "   macro avg       0.42      0.50      0.46     55392\n",
      "weighted avg       0.70      0.84      0.76     55392\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timaguettabi/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/timaguettabi/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/timaguettabi/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "approve_everyone = DummyClassifier(strategy='constant', constant = 1).fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred_dummy = approve_everyone.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred_dummy) # finally evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.a: Considering only the data itself, why do Logistic Regression and the Dummy Classifier perform the same? What is the semantic meaning for why Dummy Classifier has such high accuracy?\n",
    "\n",
    "I think it's because the data is skewed, meaning that in most cases in the data we have, we approved the person's loan which makes the Dummy classifier predictions right most of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Obtaining a Baseline\n",
    "\n",
    "### Task 4.a: Create a new balanced dataset where exactly half of the samples are rejected loan applications and half are accepted loan applications.\n",
    "(Hint: You may choose to do this iteratively with for..loops (which may take a _long_ time to run), or consider adapting the _Down-sample Majority Class_ code from [this link](https://elitedatascience.com/imbalanced-classes) - although do note that `balance` is not our target column and `49` is likely not the correct number of samples)\n",
    "\n",
    "_Show your python work in the code block below:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    60380\n",
       "0    60380\n",
       "Name: loan_approved, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # import pandas library\n",
    "from sklearn.utils import resample,shuffle\n",
    "\n",
    "df = pd.read_csv('data/home_loans_sample.csv', low_memory=False)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.loan_approved==1]\n",
    "df_minority = df[df.loan_approved==0]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, replace= False, n_samples=60380, random_state=123) \n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.loan_approved.value_counts()\n",
    "# 1    60380\n",
    "# 0    60380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.b: Below, retry training and evaluating a Logistic regression model on the updated data. What is the new performance of the model? If you were to re-run the DummyClassifier on the balanced data, what do you think would happen to its performance?\n",
    "(Hint: After balancing your original dataset, you might want to repeat the data processing, feature selection, etc. process performed in Parts 2 and 3).\n",
    "\n",
    "\n",
    "--> if we rerun the dummy classifier we'd get 0.5 accuracy because now the data is balanced (50% approved loan, 50% denied)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84532, 65) (18114, 65) (18114, 65)\n",
      "Confusion matrix:\n",
      " [[7131 1882]\n",
      " [4314 4787]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70      9013\n",
      "           1       0.72      0.53      0.61      9101\n",
      "\n",
      "    accuracy                           0.66     18114\n",
      "   macro avg       0.67      0.66      0.65     18114\n",
      "weighted avg       0.67      0.66      0.65     18114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_sex_name', 'town_name',\n",
    "'county_name']\n",
    "\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s', 'is_hoepa_loan']\n",
    "\n",
    "X_cat = df_downsampled[features_cat]\n",
    "X_num = df_downsampled[features_num]\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format\n",
    "\n",
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pd.DataFrame(one_hot.toarray(), columns=enc.get_feature_names_out())\n",
    "#X_cat_proc.head()\n",
    "\n",
    "scaled = preprocessing.scale(X_num)\n",
    "X_num_proc = pd.DataFrame(scaled, columns=features_num)\n",
    "X = pd.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X = X.fillna(0) # remove NaN values\n",
    "\n",
    "\n",
    "y = df_downsampled['loan_approved'] # target\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.30) # split out into training 70% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.50,  random_state=16) # split out into validation 15% of our data and test 15% of our data\n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # print data shape to check the sizing is correct\n",
    "\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000).fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred = model.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred))\n",
    "    \n",
    "    \n",
    "\n",
    "metrics(y_validation, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.c: Interpret the Confusion Matrix by identifying the numbers of true/false positives/negatives the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4314 1882 4787 7131\n"
     ]
    }
   ],
   "source": [
    "# Write the numbers of each in below. MAKE SURE you understand which cell in the confusion matrix has each of these.\n",
    "false_negative = 4314\n",
    "false_positive = 1882\n",
    "true_negative = 4787\n",
    "true_positive = 7131\n",
    "# these values change a little every time I rerun the code above \n",
    "\n",
    "print (false_negative, false_positive, true_negative, true_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.d: How does a *false positive* in this situation hurt _the bank_? (~20 words)\n",
    "\n",
    "This would hurt the bank by lending money to somebody who most likely would not be able to pay it back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.e: How does a *false negative* in this situation hurt _the bank_? (~20 words)\n",
    "\n",
    "The bank would suffer as a result of losing the customers to whom it would have lent money and as a result losing the potential income from interest on loans that could have been granted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.f: How does a *false positive* in this situation hurt _the customer_? (~20 words)\n",
    "\n",
    "the customer might take on loans that they would probably not be able to pay back and drown in debts as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.g: How does a *false negative* in this situation hurt _the customer_? (~20 words)\n",
    "\n",
    "the customer won't be able to get this source of financial assistance which might cause his other financial and personal issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Your Turn!\n",
    "\n",
    "### Task 5.a: Improving the model - why might this be difficult? \n",
    "\n",
    "Use your own imagination and experimentation to improve predictive performance for this task, modifying the model/algorithm choices, feature/attribute choices, and data processing however you think will be most effective. It is _very_ difficult to improve the model's performance significantly, why might that be?\n",
    "\n",
    "Important! Don't spend too much time on this portion! Consider your ability to improve the model above the baseline after Task 4.B to be only ~10% of this assignment effort, with ~5% of that given for small improvements to performance. Thus while I encourage you to experiment, do not sink excessive time into this task. The goal is to learn about the process of machine learning model development, and some of the common pitfalls!\n",
    "\n",
    "_Hint:_ Be sure to check out the other Supervised ML models/algorithms available from the [sci-kit learn documentation](https://scikit-learn.org/stable/user_guide.html#user-guide).\n",
    "\n",
    "- maybe splitting out into training 60% or less of our data instead of 70% so that we can avoid overfitting.\n",
    "- having more data but also we're constrained by the fact that we have fewer \"0\" cases than 1s as the outcome, so using more data would mean getting imbalanced, skewed data again\n",
    "- we can be more picky about the features we're feeding the model, for example race/gender shouldn't matter in deciding whether a person gets a home loan or not \n",
    "- maybe we can also experiment with different classification algorithms but I tried some from the linked website and they only seemed to perform worse than logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48304, 170) (57964, 170) (14492, 170)\n",
      "Confusion matrix:\n",
      " [[21063  7887]\n",
      " [ 3449 25565]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79     28950\n",
      "           1       0.76      0.88      0.82     29014\n",
      "\n",
      "    accuracy                           0.80     57964\n",
      "   macro avg       0.81      0.80      0.80     57964\n",
      "weighted avg       0.81      0.80      0.80     57964\n",
      "\n",
      "[[5260 2016]\n",
      " [ 803 6413]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.72      0.79      7276\n",
      "           1       0.76      0.89      0.82      7216\n",
      "\n",
      "    accuracy                           0.81     14492\n",
      "   macro avg       0.81      0.81      0.80     14492\n",
      "weighted avg       0.81      0.81      0.80     14492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_sex_name', 'town_name',\n",
    "'county_name','property_type_name', 'occupied_by_owner', 'loan_type_name',\n",
    "'loan_purpose_name', 'denial_reason_name_3', 'denial_reason_name_2', 'denial_reason_name_1',\n",
    "'co_applicant_sex_name', 'co_applicant_race_name_5',\n",
    "'co_applicant_race_name_4', 'co_applicant_race_name_3',\n",
    "'co_applicant_race_name_2', 'co_applicant_race_name_1',\n",
    "'co_applicant_ethnicity_name', 'applicant_sex_name',\n",
    "'applicant_race_name_5', 'applicant_race_name_4',\n",
    "'applicant_race_name_3', 'applicant_race_name_2',\n",
    "'applicant_race_name_1', 'applicant_ethnicity_name']\n",
    "\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s', 'is_hoepa_loan']\n",
    "\n",
    "X_cat = df_downsampled[features_cat]\n",
    "X_num = df_downsampled[features_num]\n",
    "\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format\n",
    "\n",
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pd.DataFrame(one_hot.toarray(), columns=enc.get_feature_names_out())\n",
    "#X_cat_proc.head()\n",
    "\n",
    "scaled = preprocessing.scale(X_num)\n",
    "X_num_proc = pd.DataFrame(scaled, columns=features_num)\n",
    "X = pd.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X = X.fillna(0) # remove NaN values\n",
    "y = df_downsampled['loan_approved'] # target\n",
    "\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.60) # split out into training 50% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.20) \n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # print data shape to check the sizing is correct\n",
    "    \n",
    "\n",
    "model = LogisticRegression(solver='lbfgs',max_iter=1000).fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred = model.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "predictions = model.predict(X_test)\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred))\n",
    "    \n",
    "#print(\"Accuracy:\",metrics(y_test, y_pred))\n",
    "metrics(y_validation, y_pred)\n",
    "\n",
    "#test data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.b.: What is the performance of this new model on your validation data? \n",
    "\n",
    "it has improved from 0.66 accuracy to 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.c.: How does your selected model perform on the withheld test set?\n",
    "almost same accuracy as validation data, 0.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.d: What other models did you try, and why might this one perform better than the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried Naive Bayes and decision trees, I am unsure as to why LR outperformed Naive Bayes but I think it is more suitable to use in this case than decision trees because the outcome we're trying to decide is binary, either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Your Assignment\n",
    "\n",
    "Once you've completed all of the above, you're done with assignment 1! You might want to double check that your code works like you expect. You can do this by choosing \"Restart & Run All\" in the Kernel menu. If it outputs errors, you may want to go back and check what you've done.\n",
    "\n",
    "Once you think everything is set, please upload your final notebook (with all of your code run and output showing), to Glow with filename [yourunixID]_haii21[assignmentnumber].ipynb, e.g., ikh1_haii21a1.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
